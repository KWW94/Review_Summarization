{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asd13\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.strftime('%Y-%b-%d-%H-%M-%S', time.gmtime())\n",
    "\n",
    "save_model_path = os.path.join('won', ts)\n",
    "os.makedirs('.\\\\'+save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dict_targets_Reviews.json', 'r', encoding='UTF-8') as f:\n",
    "    target_dict=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dict_inputs_Reviews.json', 'r', encoding='UTF-8') as f:\n",
    "    inputs_dict=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_unk_dic.json', 'r', encoding='UTF-8') as f:\n",
    "    word_unk_dic=json.load(f)\n",
    "\n",
    "word_list = word_unk_dic['WORD']\n",
    "\n",
    "unk_list = word_unk_dic['UNK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PAD_ = \"_PAD_\"  # 빈칸 채우는 심볼\n",
    "_STA_ = \"_STA_\"  # 디코드 입력 시퀀스의 시작 심볼\n",
    "_EOS_ = \"_EOS_\"  # 디코드 입출력 시퀀스의 종료 심볼\n",
    "_UNK_ = \"_UNK_\"  # 사전에 없는 단어를 나타내는 심볼\n",
    "\n",
    "_PAD_ID_ = 0\n",
    "_STA_ID_ = 1\n",
    "_EOS_ID_ = 2\n",
    "_UNK_ID_ = 3\n",
    "_PRE_DEFINED_ = [_PAD_ID_, _STA_ID_, _EOS_ID_, _UNK_ID_]\n",
    "\n",
    "_PRE_DEFINED_dict = {_PAD_:_PAD_ID_, _STA_:_STA_ID_, _EOS_:_EOS_ID_, _UNK_:_UNK_ID_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(seq, max_len, start=None, eos=None):\n",
    "    if start:\n",
    "        padded_seq = [_STA_] + seq\n",
    "    elif eos:\n",
    "        padded_seq = seq + [_EOS_]\n",
    "    else:\n",
    "        padded_seq = seq\n",
    "\n",
    "    if len(padded_seq) < max_len:\n",
    "        #print(len(padded_seq))\n",
    "        padded_seq = padded_seq + ([_PAD_] * (max_len - len(padded_seq)))\n",
    "        if len(padded_seq) < max_len:\n",
    "            return padded_seq[:max_len - len(padded_seq)]\n",
    "        return padded_seq\n",
    "    else:\n",
    "        if len(padded_seq) > max_len:\n",
    "            return padded_seq[:-1]\n",
    "        else : \n",
    "            return padded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_make(data):\n",
    "    \n",
    "    for index, word in enumerate(data):\n",
    "\n",
    "        if word in word_list:\n",
    "            pass\n",
    "        else:\n",
    "            data[index] = _UNK_\n",
    "            pass\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_padding(data,max_len = 0, start=None, eos=None):\n",
    "    pos_padding_dict = {}\n",
    "    for title in tqdm.tqdm(list(data.keys())):\n",
    "        if (len(data[title])>max_len):\n",
    "            data[title] = data[title][:max_len]\n",
    "        sentence = unk_make(data[title])\n",
    "        pos_padding_dict[title] = padding(sentence,max_len,start,eos)\n",
    "        \n",
    "    return pos_padding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dic_lists(data):\n",
    "    dic_list = []\n",
    "    for key in data.keys():\n",
    "        dic_list.append(data[key])\n",
    "    return dic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 568427/568427 [01:13<00:00, 7720.14it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 568427/568427 [10:57<00:00, 865.02it/s]\n"
     ]
    }
   ],
   "source": [
    "targets_pad = data_padding(target_dict,max_length,None,True)\n",
    "inputs_pad = data_padding(inputs_dict,max_length,True,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = dic_lists(inputs_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = dic_lists(targets_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in inputs:\n",
    "    if len(i) != 100:\n",
    "        print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load(\"./Word2vec_data.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index=model.wv.index2word\n",
    "\n",
    "word2index = {}\n",
    "for i,j in enumerate(model.wv.index2word):\n",
    "    word2index[j]=i\n",
    "index2word=model.wv.index2word\n",
    "\n",
    "model_embedding = []\n",
    "for index in model.wv.index2word:\n",
    "    model_embedding.append(model.wv[index])\n",
    "model_embedding = np.array(model_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_2_index(datas):\n",
    "    data_index = []\n",
    "    for data in tqdm.tqdm(datas):\n",
    "        temp = []\n",
    "        for word in data:\n",
    "            temp.append(word2index[word])\n",
    "        data_index.append(temp)\n",
    "    return data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 568427/568427 [00:07<00:00, 72372.56it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 568427/568427 [00:09<00:00, 57731.48it/s]\n"
     ]
    }
   ],
   "source": [
    "target_index = word_2_index(targets)\n",
    "input_index = word_2_index(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del targets_pad\n",
    "del inputs_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "collected = gc.collect()\n",
    "print (collected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = max_length\n",
    "embedding_size = 300\n",
    "hidden_size = 256\n",
    "word_dropout = 0.5\n",
    "latent_size = 128\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "batch_size_fit = 56\n",
    "rnn_type = 'gru'\n",
    "learning_rate = 0.001\n",
    "k = 0.002\n",
    "x0 = 8000\n",
    "vocab_size = len(index2word)\n",
    "sos_idx = word2index['_STA_']\n",
    "eos_idx = word2index['_EOS_']\n",
    "pad_idx = word2index['_PAD_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RVAE(nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_size, max_sequence_length, hidden_size, word_dropout, latent_size,\n",
    "                sos_idx, eos_idx, pad_idx, numpy_embedding,rnn_type='rnn' , num_layers=1, bidirectional=True):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.model_embedding = torch.from_numpy(numpy_embedding)\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.sos_idx = sos_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        self.latent_size = latent_size\n",
    "        self.rnn_type = rnn_type\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_size)\n",
    "        self.embedding.weight = nn.Parameter(self.model_embedding)\n",
    "        #self.outputs2vocab = nn.Linear(hidden_size * (2 if bidirectional else 1), vocab_size)\n",
    "        self.encoder = Encoder(vocab_size = vocab_size,embedding_size = embedding_size, hidden_size = hidden_size, num_layers = num_layers, bidirectional = bidirectional,latent_size = latent_size,rnn_type = rnn_type).cuda()\n",
    "        self.decoder = Decoder(vocab_size = vocab_size,embedding_size = embedding_size, hidden_size = hidden_size, num_layers = num_layers, bidirectional = bidirectional,latent_size = latent_size,rnn_type = rnn_type, word_dropout=word_dropout).cuda()\n",
    "    \n",
    "    def forward(self,x,length): \n",
    "        #print(x.size())\n",
    "        batch_size = x.size(0)\n",
    "        sorted_lengths, sorted_idx = torch.sort(length, descending=True)\n",
    "        input_sequence = x[sorted_idx.cuda()]\n",
    "        input_embedding = self.embedding(input_sequence)\n",
    "        \n",
    "        packed_input = rnn_utils.pack_padded_sequence(input_embedding, sorted_lengths.tolist(), batch_first=True)\n",
    "\n",
    "        mu,logvar,reparam = self.encoder(packed_input)\n",
    "        logp,outputs  = self.decoder(input_embedding, reparam, sorted_lengths, sorted_idx)\n",
    "        \n",
    "        \n",
    "        return logp, mu, logvar, reparam, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_size, hidden_size, latent_size, bidirectional=True, num_layers = 1,rnn_type='rnn'):\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.latent_size = latent_size\n",
    "        self.rnn_type = rnn_type\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.rnn_type == 'rnn':\n",
    "            rnn = nn.RNN\n",
    "        elif self.rnn_type == 'gru':\n",
    "            rnn = nn.GRU\n",
    "        elif self.rnn_type =='lstm':\n",
    "            rnn = nn.LSTM\n",
    "        else:\n",
    "            raise ValueError()\n",
    "            \n",
    "        \n",
    "        self.encoder = rnn(self.embedding_size, self.hidden_size, num_layers = self.num_layers, bidirectional = self.bidirectional, batch_first = True)\n",
    "        \n",
    "        self.hidden_factor = (2 if self.bidirectional else 1) * self.num_layers\n",
    "        \n",
    "        self.hidden2mean = nn.Linear(self.hidden_size* self.hidden_factor, self.latent_size)\n",
    "        self.hidden2logv = nn.Linear(self.hidden_size* self.hidden_factor, self.latent_size)\n",
    "                        \n",
    "    \n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        \n",
    "        eps = torch.FloatTensor(std.size()).normal_()\n",
    "        #print(eps)\n",
    "        eps = Variable(eps).cuda()\n",
    "        \n",
    "        return eps.mul(std).add_(mu)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        _,hidden = self.encoder(x)\n",
    "        \n",
    "        if self.bidirectional or self.num_layers > 1:\n",
    "            # flatten hidden state\n",
    "            hidden = hidden.view(batch_size, self.hidden_size*self.hidden_factor)\n",
    "        else:\n",
    "            hidden = hidden.squeeze()\n",
    "\n",
    "        mu = self.hidden2mean(hidden)\n",
    "        \n",
    "        logvar = self.hidden2logv(hidden)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        #reparam = self.reparametrize(mu,logvar)\n",
    "        z = Variable(torch.randn([batch_size, self.latent_size])).cuda()\n",
    "        z = z * std + mu\n",
    "        \n",
    "        \n",
    "        return mu,logvar,z#,reparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_size, hidden_size, latent_size, bidirectional=True, num_layers = 1,rnn_type='rnn',word_dropout = 0.5):\n",
    "        super(Decoder,self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.latent_size = latent_size\n",
    "        self.rnn_type = rnn_type\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.rnn_type == 'rnn':\n",
    "            rnn = nn.RNN\n",
    "        elif self.rnn_type == 'gru':\n",
    "            rnn = nn.GRU\n",
    "        elif self.rnn_type =='lstm':\n",
    "            rnn = nn.LSTM\n",
    "        else:\n",
    "            raise ValueError()\n",
    "            \n",
    "        self.hidden_factor = (2 if self.bidirectional else 1) * self.num_layers            \n",
    "        self.latent2hidden = nn.Linear(latent_size, hidden_size * self.hidden_factor)\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_size)\n",
    "        self.word_dropout = nn.Dropout(p=word_dropout)\n",
    "        \n",
    "        self.decoder = rnn(embedding_size, hidden_size, num_layers=num_layers, bidirectional=self.bidirectional, batch_first=True)\n",
    "        self.decoder2outputs = nn.Linear(hidden_size * (2 if bidirectional else 1), hidden_size * (2 if bidirectional else 1))\n",
    "        self.outputs2vocab = nn.Linear(hidden_size * (2 if bidirectional else 1), vocab_size)\n",
    "        \n",
    "    def forward(self,x,z,sorted_lengths,sorted_idx):\n",
    "\n",
    "        hidden = self.latent2hidden(z)\n",
    "        if self.bidirectional or self.num_layers > 1:\n",
    "            # unflatten hidden state\n",
    "            hidden = hidden.view(self.hidden_factor, batch_size, self.hidden_size)\n",
    "        else:\n",
    "            hidden = hidden.unsqueeze(0)\n",
    "        input_embedding = self.word_dropout(x)\n",
    "        packed_input = rnn_utils.pack_padded_sequence(input_embedding, sorted_lengths.tolist(), batch_first=True)\n",
    "        outputs,_ = self.decoder(packed_input, hidden)\n",
    "        \n",
    "        padded_outputs = rnn_utils.pad_packed_sequence(outputs, batch_first=True)[0]\n",
    "        padded_outputs = padded_outputs.contiguous()\n",
    "        _,reversed_idx = torch.sort(sorted_idx.cuda())\n",
    "        padded_outputs = padded_outputs[reversed_idx]\n",
    "        b,s,_ = padded_outputs.size()\n",
    "        \n",
    "        #print(padded_outputs.view(-1, padded_outputs.size(2)).size())\n",
    "        output_vocab = self.outputs2vocab(self.decoder2outputs(padded_outputs.view(-1, padded_outputs.size(2))))\n",
    "        logp = nn.functional.log_softmax(output_vocab, dim=-1)\n",
    "        logp = logp.view(b, s, self.embedding.num_embeddings)\n",
    "        \n",
    "        return logp,padded_outputs.view(-1, padded_outputs.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvae=RVAE(vocab_size, embedding_size, max_sequence_length, hidden_size, word_dropout, latent_size,sos_idx, eos_idx, pad_idx , numpy_embedding = model_embedding, num_layers=num_layers ,rnn_type='gru',bidirectional= False).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_anneal_function(anneal_function, step, k, x0):\n",
    "    if anneal_function == 'logistic':\n",
    "        return float(1/(1+np.exp(-k*(step-x0))))\n",
    "    elif anneal_function == 'linear':\n",
    "        return min(1, step/x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLL = torch.nn.NLLLoss(size_average=False, ignore_index = pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logp, target, length, mean, logv, anneal_function, step, k, x0):\n",
    "\n",
    "    # cut-off unnecessary padding from target, and flatten\n",
    "    target = target[:, :torch.max(length)].contiguous().view(-1)\n",
    "    logp = logp.view(-1, logp.size(2))\n",
    "\n",
    "    # Negative Log Likelihood\n",
    "    NLL_loss = NLL(logp, target)\n",
    "\n",
    "    # KL Divergence\n",
    "    KL_loss = -0.5 * torch.sum(1 + logv - mean.pow(2) - logv.exp())\n",
    "    KL_weight = kl_anneal_function(anneal_function, step, k, x0)\n",
    "\n",
    "    return NLL_loss, KL_loss, KL_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(rvae.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RVAE(\n",
       "  (embedding): Embedding(60923, 300)\n",
       "  (encoder): Encoder(\n",
       "    (encoder): GRU(300, 256, num_layers=2, batch_first=True)\n",
       "    (hidden2mean): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (hidden2logv): Linear(in_features=512, out_features=128, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (latent2hidden): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (embedding): Embedding(60923, 300)\n",
       "    (word_dropout): Dropout(p=0.5)\n",
       "    (decoder): GRU(300, 256, num_layers=2, batch_first=True)\n",
       "    (decoder2outputs): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (outputs2vocab): Linear(in_features=256, out_features=60923, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(batch_size,input_var,target_var,length_var):\n",
    "    start = 0\n",
    "    end = batch_size\n",
    "    #if len(input_var)%32 != 0:\n",
    "    while end < len(input_var):\n",
    "        batch_input = input_var[start:end]\n",
    "        batch_target = target_var[start:end]\n",
    "        batch_length = length_var[start:end]\n",
    "        temp = end\n",
    "        end  = end + batch_size\n",
    "        start = temp\n",
    "        yield batch_input, batch_target, batch_length\n",
    "        \n",
    "    if end >= len(input_var):\n",
    "        batch_input  = input_var[start:]\n",
    "        batch_target = target_var[start:]\n",
    "        batch_length = length_var[start:]\n",
    "        yield batch_input, batch_target, batch_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_index = int(len(input_index)*0.6)\n",
    "test_index = int(len(input_index)*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_len = []\n",
    "for sentence in input_index:\n",
    "    inputs_len.append(len(sentence) - sentence.count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/6090, Loss   26.2992, NLL-Loss   26.2991, KL-Loss  135.8435, KL-Weight  0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asd13\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:159: UserWarning: Couldn't retrieve source code for container of type RVAE. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\asd13\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:159: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\asd13\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:159: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000/6090, Loss   34.9820, NLL-Loss   34.9808, KL-Loss  201.1415, KL-Weight  0.000\n",
      "Batch 3000/6090, Loss   33.3733, NLL-Loss   33.3623, KL-Loss  243.6513, KL-Weight  0.000\n",
      "Batch 4000/6090, Loss   28.4656, NLL-Loss   28.4005, KL-Loss  194.5762, KL-Weight  0.000\n",
      "Batch 5000/6090, Loss   32.3901, NLL-Loss   32.1686, KL-Loss   89.7368, KL-Weight  0.002\n",
      "Batch 6000/6090, Loss   27.0635, NLL-Loss   26.6757, KL-Loss   21.6028, KL-Weight  0.018\n",
      "Batch 6090/6090, Loss   28.8794, NLL-Loss   28.4053, KL-Loss   22.1344, KL-Weight  0.021\n",
      "Valid Batch 1000/2030, Loss   30.8294, NLL-Loss   30.3533, KL-Loss   22.1478, KL-Weight  0.021\n",
      "Valid Batch 2000/2030, Loss   28.4697, NLL-Loss   27.9569, KL-Loss   23.8510, KL-Weight  0.021\n",
      "Model saved at won\\2018-Jun-22-08-59-57\\E0.pytorch\n",
      "Epoch 00/10, Valid Mean ELBO   30.2034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|███████▉                                                                       | 1/10 [25:35<3:50:23, 1535.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/6090, Loss   25.2133, NLL-Loss   24.4313, KL-Loss    5.6083, KL-Weight  0.139\n",
      "Batch 2000/6090, Loss   35.1825, NLL-Loss   34.5177, KL-Loss    1.2201, KL-Weight  0.545\n",
      "Batch 3000/6090, Loss   33.9646, NLL-Loss   33.6619, KL-Loss    0.3369, KL-Weight  0.898\n",
      "Batch 4000/6090, Loss   29.9831, NLL-Loss   29.7806, KL-Loss    0.2056, KL-Weight  0.985\n",
      "Batch 5000/6090, Loss   34.1459, NLL-Loss   34.0424, KL-Loss    0.1036, KL-Weight  0.998\n",
      "Batch 6000/6090, Loss   27.7255, NLL-Loss   27.6682, KL-Loss    0.0573, KL-Weight  1.000\n",
      "Batch 6090/6090, Loss   29.7764, NLL-Loss   29.7113, KL-Loss    0.0651, KL-Weight  1.000\n",
      "Valid Batch 1000/2030, Loss   31.7401, NLL-Loss   31.4930, KL-Loss    0.2471, KL-Weight  1.000\n",
      "Valid Batch 2000/2030, Loss   30.5516, NLL-Loss   30.4076, KL-Loss    0.1440, KL-Weight  1.000\n",
      "Model saved at won\\2018-Jun-22-08-59-57\\E1.pytorch\n",
      "Epoch 01/10, Valid Mean ELBO   30.8818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|███████████████▊                                                               | 2/10 [51:14<3:24:56, 1537.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/6090, Loss   26.0634, NLL-Loss   25.9629, KL-Loss    0.1005, KL-Weight  1.000\n",
      "Batch 2000/6090, Loss   34.9210, NLL-Loss   34.6772, KL-Loss    0.2438, KL-Weight  1.000\n",
      "Batch 3000/6090, Loss   32.4568, NLL-Loss   32.4262, KL-Loss    0.0306, KL-Weight  1.000\n",
      "Batch 4000/6090, Loss   29.4672, NLL-Loss   29.3649, KL-Loss    0.1023, KL-Weight  1.000\n",
      "Batch 5000/6090, Loss   33.6877, NLL-Loss   33.5749, KL-Loss    0.1127, KL-Weight  1.000\n",
      "Batch 6000/6090, Loss   27.5254, NLL-Loss   27.4743, KL-Loss    0.0511, KL-Weight  1.000\n",
      "Batch 6090/6090, Loss   29.5697, NLL-Loss   29.5131, KL-Loss    0.0566, KL-Weight  1.000\n",
      "Valid Batch 1000/2030, Loss   31.6462, NLL-Loss   31.3941, KL-Loss    0.2522, KL-Weight  1.000\n",
      "Valid Batch 2000/2030, Loss   29.8575, NLL-Loss   29.7213, KL-Loss    0.1362, KL-Weight  1.000\n",
      "Model saved at won\\2018-Jun-22-08-59-57\\E2.pytorch\n",
      "Epoch 02/10, Valid Mean ELBO   31.0470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███████████████████████                                                      | 3/10 [1:16:59<2:59:39, 1539.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/6090, Loss   25.8831, NLL-Loss   25.8117, KL-Loss    0.0714, KL-Weight  1.000\n",
      "Batch 2000/6090, Loss   33.2400, NLL-Loss   32.8986, KL-Loss    0.3414, KL-Weight  1.000\n",
      "Batch 3000/6090, Loss   30.1591, NLL-Loss   30.1129, KL-Loss    0.0462, KL-Weight  1.000\n",
      "Batch 4000/6090, Loss   29.6131, NLL-Loss   29.5157, KL-Loss    0.0974, KL-Weight  1.000\n",
      "Batch 5000/6090, Loss   32.6769, NLL-Loss   32.5440, KL-Loss    0.1330, KL-Weight  1.000\n",
      "Batch 6000/6090, Loss   27.1206, NLL-Loss   27.0327, KL-Loss    0.0880, KL-Weight  1.000\n",
      "Batch 6090/6090, Loss   29.4946, NLL-Loss   29.4172, KL-Loss    0.0775, KL-Weight  1.000\n",
      "Valid Batch 1000/2030, Loss   32.1239, NLL-Loss   31.8099, KL-Loss    0.3140, KL-Weight  1.000\n",
      "Valid Batch 2000/2030, Loss   29.6150, NLL-Loss   29.4274, KL-Loss    0.1876, KL-Weight  1.000\n",
      "Model saved at won\\2018-Jun-22-08-59-57\\E3.pytorch\n",
      "Epoch 03/10, Valid Mean ELBO   31.1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|██████████████████████████████▊                                              | 4/10 [1:42:43<2:34:05, 1540.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/6090, Loss   25.8131, NLL-Loss   25.7308, KL-Loss    0.0824, KL-Weight  1.000\n",
      "Batch 2000/6090, Loss   32.4201, NLL-Loss   32.1039, KL-Loss    0.3162, KL-Weight  1.000\n",
      "Batch 3000/6090, Loss   28.4752, NLL-Loss   28.4266, KL-Loss    0.0487, KL-Weight  1.000\n",
      "Batch 4000/6090, Loss   29.2516, NLL-Loss   29.1114, KL-Loss    0.1402, KL-Weight  1.000\n",
      "Batch 5000/6090, Loss   32.2883, NLL-Loss   32.1074, KL-Loss    0.1809, KL-Weight  1.000\n",
      "Batch 6000/6090, Loss   26.8596, NLL-Loss   26.7579, KL-Loss    0.1017, KL-Weight  1.000\n",
      "Batch 6090/6090, Loss   29.4204, NLL-Loss   29.3280, KL-Loss    0.0924, KL-Weight  1.000\n",
      "Valid Batch 1000/2030, Loss   32.2172, NLL-Loss   31.9556, KL-Loss    0.2616, KL-Weight  1.000\n",
      "Valid Batch 2000/2030, Loss   28.9914, NLL-Loss   28.7954, KL-Loss    0.1960, KL-Weight  1.000\n",
      "Model saved at won\\2018-Jun-22-08-59-57\\E4.pytorch\n",
      "Epoch 04/10, Valid Mean ELBO   31.1400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████████████████████▌                                      | 5/10 [2:08:30<2:08:30, 1542.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/6090, Loss   25.7105, NLL-Loss   25.6395, KL-Loss    0.0710, KL-Weight  1.000\n",
      "Batch 2000/6090, Loss   31.3473, NLL-Loss   30.9061, KL-Loss    0.4412, KL-Weight  1.000\n",
      "Batch 3000/6090, Loss   27.7331, NLL-Loss   27.6804, KL-Loss    0.0527, KL-Weight  1.000\n",
      "Batch 4000/6090, Loss   29.3257, NLL-Loss   29.1634, KL-Loss    0.1623, KL-Weight  1.000\n",
      "Batch 5000/6090, Loss   31.5043, NLL-Loss   31.3431, KL-Loss    0.1612, KL-Weight  1.000\n",
      "Batch 6000/6090, Loss   26.7720, NLL-Loss   26.6613, KL-Loss    0.1106, KL-Weight  1.000\n",
      "Batch 6090/6090, Loss   29.3182, NLL-Loss   29.2131, KL-Loss    0.1051, KL-Weight  1.000\n",
      "Valid Batch 1000/2030, Loss   32.6471, NLL-Loss   32.3611, KL-Loss    0.2860, KL-Weight  1.000\n",
      "Valid Batch 2000/2030, Loss   28.8848, NLL-Loss   28.6939, KL-Loss    0.1909, KL-Weight  1.000\n",
      "Model saved at won\\2018-Jun-22-08-59-57\\E5.pytorch\n",
      "Epoch 05/10, Valid Mean ELBO   31.1695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████▏                              | 6/10 [2:34:14<1:42:49, 1542.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/6090, Loss   25.4232, NLL-Loss   25.3315, KL-Loss    0.0917, KL-Weight  1.000\n",
      "Batch 2000/6090, Loss   31.0069, NLL-Loss   30.6073, KL-Loss    0.3996, KL-Weight  1.000\n",
      "Batch 3000/6090, Loss   26.6676, NLL-Loss   26.6094, KL-Loss    0.0581, KL-Weight  1.000\n",
      "Batch 4000/6090, Loss   29.6052, NLL-Loss   29.4148, KL-Loss    0.1904, KL-Weight  1.000\n",
      "Batch 5000/6090, Loss   31.3052, NLL-Loss   31.0980, KL-Loss    0.2072, KL-Weight  1.000\n",
      "Batch 6000/6090, Loss   26.4837, NLL-Loss   26.3557, KL-Loss    0.1279, KL-Weight  1.000\n",
      "Batch 6090/6090, Loss   28.8045, NLL-Loss   28.6808, KL-Loss    0.1237, KL-Weight  1.000\n",
      "Valid Batch 1000/2030, Loss   32.5082, NLL-Loss   32.1918, KL-Loss    0.3164, KL-Weight  1.000\n",
      "Valid Batch 2000/2030, Loss   28.2941, NLL-Loss   28.0674, KL-Loss    0.2267, KL-Weight  1.000\n",
      "Model saved at won\\2018-Jun-22-08-59-57\\E6.pytorch\n",
      "Epoch 06/10, Valid Mean ELBO   31.1795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████████████████████████████████▉                       | 7/10 [3:00:02<1:17:09, 1543.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/6090, Loss   25.5943, NLL-Loss   25.5004, KL-Loss    0.0938, KL-Weight  1.000\n",
      "Batch 2000/6090, Loss   30.5819, NLL-Loss   30.1929, KL-Loss    0.3890, KL-Weight  1.000\n",
      "Batch 3000/6090, Loss   26.2378, NLL-Loss   26.1759, KL-Loss    0.0619, KL-Weight  1.000\n",
      "Batch 4000/6090, Loss   29.4084, NLL-Loss   29.2435, KL-Loss    0.1649, KL-Weight  1.000\n",
      "Batch 5000/6090, Loss   30.5386, NLL-Loss   30.3519, KL-Loss    0.1867, KL-Weight  1.000\n",
      "Batch 6000/6090, Loss   26.5869, NLL-Loss   26.4388, KL-Loss    0.1481, KL-Weight  1.000\n",
      "Batch 6090/6090, Loss   28.9019, NLL-Loss   28.7832, KL-Loss    0.1187, KL-Weight  1.000\n",
      "Valid Batch 1000/2030, Loss   32.6744, NLL-Loss   32.3857, KL-Loss    0.2886, KL-Weight  1.000\n",
      "Valid Batch 2000/2030, Loss   28.1699, NLL-Loss   27.9264, KL-Loss    0.2436, KL-Weight  1.000\n",
      "Model saved at won\\2018-Jun-22-08-59-57\\E7.pytorch\n",
      "Epoch 07/10, Valid Mean ELBO   31.1888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████▏               | 8/10 [3:26:24<51:36, 1548.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/6090, Loss   25.3644, NLL-Loss   25.2616, KL-Loss    0.1028, KL-Weight  1.000\n",
      "Batch 2000/6090, Loss   29.8520, NLL-Loss   29.3671, KL-Loss    0.4850, KL-Weight  1.000\n",
      "Batch 3000/6090, Loss   25.9959, NLL-Loss   25.9196, KL-Loss    0.0763, KL-Weight  1.000\n",
      "Batch 4000/6090, Loss   29.4884, NLL-Loss   29.3292, KL-Loss    0.1591, KL-Weight  1.000\n",
      "Batch 5000/6090, Loss   30.7202, NLL-Loss   30.4387, KL-Loss    0.2815, KL-Weight  1.000\n",
      "Batch 6000/6090, Loss   26.4294, NLL-Loss   26.2780, KL-Loss    0.1514, KL-Weight  1.000\n",
      "Batch 6090/6090, Loss   28.8355, NLL-Loss   28.6927, KL-Loss    0.1428, KL-Weight  1.000\n",
      "Valid Batch 1000/2030, Loss   32.8058, NLL-Loss   32.5136, KL-Loss    0.2922, KL-Weight  1.000\n",
      "Valid Batch 2000/2030, Loss   28.4514, NLL-Loss   28.1924, KL-Loss    0.2590, KL-Weight  1.000\n",
      "Model saved at won\\2018-Jun-22-08-59-57\\E8.pytorch\n",
      "Epoch 08/10, Valid Mean ELBO   31.1996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|███████████████████████████████████████████████████████████████████████        | 9/10 [3:52:51<25:52, 1552.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000/6090, Loss   25.5517, NLL-Loss   25.4366, KL-Loss    0.1151, KL-Weight  1.000\n",
      "Batch 2000/6090, Loss   29.8253, NLL-Loss   29.4247, KL-Loss    0.4007, KL-Weight  1.000\n",
      "Batch 3000/6090, Loss   25.7330, NLL-Loss   25.6541, KL-Loss    0.0789, KL-Weight  1.000\n",
      "Batch 4000/6090, Loss   29.1125, NLL-Loss   28.9454, KL-Loss    0.1671, KL-Weight  1.000\n",
      "Batch 5000/6090, Loss   30.1598, NLL-Loss   29.8506, KL-Loss    0.3092, KL-Weight  1.000\n",
      "Batch 6000/6090, Loss   26.2937, NLL-Loss   26.1478, KL-Loss    0.1459, KL-Weight  1.000\n",
      "Batch 6090/6090, Loss   28.4183, NLL-Loss   28.2854, KL-Loss    0.1329, KL-Weight  1.000\n",
      "Valid Batch 1000/2030, Loss   33.2028, NLL-Loss   32.9127, KL-Loss    0.2901, KL-Weight  1.000\n",
      "Valid Batch 2000/2030, Loss   28.1908, NLL-Loss   27.9192, KL-Loss    0.2715, KL-Weight  1.000\n",
      "Model saved at won\\2018-Jun-22-08-59-57\\E9.pytorch\n",
      "Epoch 09/10, Valid Mean ELBO   31.2087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 10/10 [4:19:05<00:00, 1554.53s/it]\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "\n",
    "var_avg_losses = []\n",
    "\n",
    "train_losses = []\n",
    "var_losses = []\n",
    "\n",
    "var_NLL_losses = []\n",
    "var_KL_losses = []\n",
    "\n",
    "train_NLL_losses = []\n",
    "train_KL_losses = []\n",
    "train_KL_weights = []\n",
    "\n",
    "iteration = 0\n",
    "for epoch in tqdm.tqdm(range(epochs)):\n",
    "    iteration = 0\n",
    "    for batch_x, batch_y, batch_len in batch(batch_size_fit, input_index[:var_index], target_index[:var_index], \\\n",
    "                                             inputs_len[:var_index]):\n",
    "        iteration = iteration + 1\n",
    "        \n",
    "        x_ = Variable(torch.cuda.LongTensor(batch_x))\n",
    "        y_ = Variable(torch.cuda.LongTensor(batch_y))\n",
    "        batch_size = x_.size(0)\n",
    "        length = torch.cuda.LongTensor(batch_len)\n",
    "\n",
    "        logp, mean, logv, z, outputs=rvae(x_,length)\n",
    "\n",
    "        NLL_loss, KL_loss, KL_weight = loss_fn(logp, y_, length, mean, logv, 'logistic', step, k, x0)\n",
    "        \n",
    "        loss = (NLL_loss+KL_loss*KL_weight)/batch_size #(NLL_loss/batch_size)\n",
    "\n",
    "\n",
    "        train_losses.append(float(loss.cpu().data))\n",
    "        train_NLL_losses.append(NLL_loss.data[0]/batch_size)\n",
    "        train_KL_losses.append(KL_loss.data[0]/batch_size)\n",
    "        train_KL_weights.append(KL_weight/batch_size)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "\n",
    "        if iteration % 1000 == 0 or iteration == (len(input_index[:var_index])-1)//batch_size:\n",
    "            print(\"Batch %04d/%i, Loss %9.4f, NLL-Loss %9.4f, KL-Loss %9.4f, KL-Weight %6.3f\"\n",
    "        %( iteration, (len(input_index[:var_index])-1)//batch_size_fit, loss.data[0], NLL_loss.data[0]/batch_size, KL_loss.data[0]/batch_size, KL_weight))\n",
    "            np.savez(L=train_losses,file='train_loss.npz')\n",
    "            np.savez(L=train_NLL_losses,file='train_NLL_losses.npz')\n",
    "            np.savez(L=train_KL_losses,file='train_KL_losses.npz') \n",
    "            np.savez(L=train_KL_weights,file='train_KL_weights.npz') \n",
    "            checkpoint_path = os.path.join(save_model_path, \"E%i.pytorch\"%(epoch))\n",
    "            torch.save(rvae, checkpoint_path)\n",
    "            \n",
    "    iteration = 0\n",
    "    for batch_x, batch_y, batch_len in batch(batch_size_fit, input_index[var_index:test_index], target_index[var_index:test_index], \\\n",
    "                                             inputs_len[var_index:test_index]):\n",
    "        iteration = iteration + 1\n",
    "        \n",
    "        x_ = Variable(torch.cuda.LongTensor(batch_x))\n",
    "        y_ = Variable(torch.cuda.LongTensor(batch_y))\n",
    "        batch_size = x_.size(0)\n",
    "        length = torch.cuda.LongTensor(batch_len)\n",
    "\n",
    "        logp, mean, logv, z, outputs=rvae(x_,length)\n",
    "\n",
    "        NLL_loss, KL_loss, KL_weight = loss_fn(logp, y_, length, mean, logv, 'logistic', step, k, x0)\n",
    "\n",
    "        loss = (NLL_loss + KL_loss*KL_weight)/batch_size#(NLL_loss/batch_size)\n",
    "\n",
    "\n",
    "        var_losses.append(float(loss.cpu().data))\n",
    "        var_NLL_losses.append(NLL_loss.data[0]/batch_size)\n",
    "        var_KL_losses.append(KL_loss.data[0]/batch_size)\n",
    "\n",
    "\n",
    "\n",
    "        if iteration % 1000 == 0 or iteration == (len(input_index[var_index:])-1)//batch_size:\n",
    "            print(\"Valid Batch %04d/%i, Loss %9.4f, NLL-Loss %9.4f, KL-Loss %9.4f, KL-Weight %6.3f\"\n",
    "        %( iteration, (len(input_index[var_index:test_index])-1)//batch_size_fit, loss.data[0], NLL_loss.data[0]/batch_size, KL_loss.data[0]/batch_size, KL_weight))\n",
    "            np.savez(L=var_losses,file='var_loss.npz')\n",
    "            np.savez(L=var_NLL_losses,file='var_NLL_loss.npz')\n",
    "            np.savez(L=var_KL_losses,file='var_KL_loss.npz') \n",
    "            checkpoint_path = os.path.join(save_model_path, \"E%i.pytorch\"%(epoch))\n",
    "            torch.save(rvae, checkpoint_path)\n",
    "            \n",
    "    checkpoint_path = os.path.join(save_model_path, \"E%i.pytorch\"%(epoch))\n",
    "    torch.save(rvae, checkpoint_path)\n",
    "    print(\"Model saved at %s\"%checkpoint_path)\n",
    "    print(\"Epoch %02d/%i, Valid Mean ELBO %9.4f\"%( epoch, epochs, np.mean(np.array(var_losses))))\n",
    "    var_avg_losses.append(np.mean(np.array(var_losses)))\n",
    "    np.savez(L=var_avg_losses,file='var_avg_loss.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'won\\\\2018-Jun-20-02-18-18\\\\E9.pytorch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------model restored--------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RVAE(\n",
       "  (embedding): Embedding(60923, 300)\n",
       "  (encoder): Encoder(\n",
       "    (encoder): GRU(300, 256, num_layers=2, batch_first=True)\n",
       "    (hidden2mean): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (hidden2logv): Linear(in_features=512, out_features=128, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (latent2hidden): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (embedding): Embedding(60923, 300)\n",
       "    (word_dropout): Dropout(p=0.5)\n",
       "    (decoder): GRU(300, 256, num_layers=2, batch_first=True)\n",
       "    (decoder2outputs): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (outputs2vocab): Linear(in_features=256, out_features=60923, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    rvae = torch.load(checkpoint_path)\n",
    "    print(\"\\n--------model restored--------\\n\")\n",
    "except:\n",
    "    print(\"\\n--------model not restored--------\\n\")\n",
    "    pass\n",
    "rvae.cuda()\n",
    "rvae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(batch, z):\n",
    "    \n",
    "    hidden = rvae.decoder.latent2hidden(z)\n",
    "    hidden = hidden.view(rvae.decoder.hidden_factor, batch_size, rvae.hidden_size)\n",
    "    \n",
    "    t = 0\n",
    "    generations=torch.cuda.FloatTensor(batch_size, max_sequence_length).fill_(pad_idx).long()\n",
    "    \n",
    "    while(t<max_sequence_length ):\n",
    "        if t == 0:\n",
    "            input_sequence = Variable(torch.Tensor(batch_size).fill_(sos_idx).long()).cuda()\n",
    "        input_sequence = input_sequence.unsqueeze(1)\n",
    "        input_embedding = rvae.embedding(input_sequence)\n",
    "        output, hidden = rvae.decoder.decoder(input_embedding, hidden)\n",
    "        logits = rvae.decoder.outputs2vocab(output)\n",
    "        samples = torch.topk(logits,1,dim=-1)[1].squeeze()\n",
    "        input_sequence = samples.squeeze()\n",
    "        generations[:,t] = input_sequence.data\n",
    "        if ((torch.sum(input_sequence)/input_sequence.size(0)) == eos_idx).cpu().data.numpy():\n",
    "            break\n",
    "        t+=1\n",
    "    return generations\n",
    "\n",
    "def print_inference(generations):\n",
    "\n",
    "\n",
    "    w2i, i2w = word2index, index2word\n",
    "\n",
    "    samples = generations.cpu().numpy()\n",
    "\n",
    "    sent_str = [str()]*len(samples)\n",
    "\n",
    "    for i, sent in enumerate(samples):\n",
    "        for word_id in sent:\n",
    "            if word_id == w2i['_PAD_']: \n",
    "                break\n",
    "            elif word_id == eos_idx:\n",
    "                break\n",
    "            sent_str[i] += i2w[word_id] + \" \"\n",
    "        sent_str[i] = sent_str[i].strip()\n",
    "    return sent_str\n",
    "\n",
    "def print_input(generations):\n",
    "\n",
    "\n",
    "    w2i, i2w = word2index, index2word\n",
    "\n",
    "    samples = generations.cpu().data.numpy()\n",
    "\n",
    "    sent_str = [str()]*len(samples)\n",
    "\n",
    "    for i, sent in enumerate(samples):\n",
    "        for word_id in sent:\n",
    "            if word_id == pad_idx: \n",
    "                break\n",
    "            elif word_id == eos_idx:\n",
    "                break\n",
    "            sent_str[i] += i2w[word_id] + \" \"\n",
    "        sent_str[i] = sent_str[i].strip()\n",
    "    return sent_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_len = []\n",
    "for sentence in input_index[:15]:\n",
    "    inputs_len.append(len(sentence) - sentence.count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = Variable(torch.cuda.LongTensor(input_index[:15]))\n",
    "y_ = Variable(torch.cuda.LongTensor(target_index[0]))\n",
    "batch_size = x_.size(0)\n",
    "length = torch.cuda.LongTensor(inputs_len)\n",
    "\n",
    "logp, mean, logv, z, outputs=rvae(x_,length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_print = []\n",
    "for i in target_index[:15]:\n",
    "    temp = []\n",
    "    for j in i :\n",
    "        if j == 0:\n",
    "            break\n",
    "        else :\n",
    "            temp.append(index2word[j])\n",
    "    target_print.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: _STA_ i have bought several of the vitality canned dog food products and have found them all to be of good quality . the product looks more like a stew than a processed meat and it smells better . my labrador is finicky and she appreciates this product better than most .\n",
      "inference: \n",
      "target: good quality dog food _EOS_\n",
      "input: _STA_ product arrived labeled as jumbo salted peanuts ... the peanuts were actually small sized unsalted . not sure if this was an error or if the vendor intended to represent the product as `` jumbo '' .\n",
      "inference: \n",
      "target: not as advertised _EOS_\n",
      "input: _STA_ this is a confection that has been around a few centuries . it is a light , pillowy citrus gelatin with nuts - in this case filberts . and it is cut into tiny squares and then liberally coated with powdered sugar . and it is a tiny mouthful of heaven . not too chewy , and very flavorful . i highly recommend this yummy treat . if you are familiar with the story of c.s . lewis ' `` the lion , the witch , and the wardrobe '' - this is the treat that _UNK_ edmund into\n",
      "inference: \n",
      "target: `` delight '' says it all _EOS_\n",
      "input: _STA_ if you are looking for the secret ingredient in robitussin i believe i have found it . i got this in addition to the root beer extract i ordered ( which was good ) and made some cherry soda . the flavor is very medicinal .\n",
      "inference: \n",
      "target: cough medicine _EOS_\n",
      "input: _STA_ great taffy at a great price . there was a wide assortment of yummy taffy . delivery was very quick . if your a taffy lover , this is a deal .\n",
      "inference: fineness planning plays\n",
      "target: great taffy _EOS_\n",
      "input: _STA_ i got a wild hair for taffy and ordered this five pound bag . the taffy was all very enjoyable with many flavors : watermelon , root beer , melon , peppermint , grape , etc . my only complaint is there was a bit too much _UNK_ licorice-flavored pieces ( just not my particular favorites ) . between me , my kids , and my husband , this lasted only two weeks ! i would recommend this brand of taffy -- it was a delightful treat .\n",
      "inference: fineness planning plays\n",
      "target: nice taffy _EOS_\n",
      "input: _STA_ this saltwater taffy had great flavors and was very soft and chewy . each candy was individually wrapped well . none of the candies were stuck together , which did happen in the expensive version , fralinger 's . would highly recommend this candy ! i served it at a _UNK_ party and everyone loved it !\n",
      "inference: \n",
      "target: great ! just as good as the expensive brands ! _EOS_\n",
      "input: _STA_ this taffy is so good . it is very soft and chewy . the flavors are amazing . i would definitely recommend you buying it . very satisfying ! !\n",
      "inference: \n",
      "target: wonderful , tasty taffy _EOS_\n",
      "input: _STA_ right now i 'm mostly just sprouting this so my cats can eat the grass . they love it . i rotate it around with wheatgrass and rye too\n",
      "inference: \n",
      "target: yay barley _EOS_\n",
      "input: _STA_ this is a very healthy dog food . good for their digestion . also good for small puppies . my dog eats her required amount at every feeding .\n",
      "inference: \n",
      "target: healthy dog food _EOS_\n",
      "input: _STA_ i do n't know if it 's the cactus or the tequila or just the unique combination of ingredients , but the flavour of this hot sauce makes it one of a kind ! we picked up a bottle once on a trip we were on and brought it back home with us and were totally blown away ! when we realized that we simply could n't find it anywhere in our city we were _UNK_ < br / > < br / > now , because of the magic of the internet , we have a case of\n",
      "inference: \n",
      "target: the best hot sauce in the world _EOS_\n",
      "input: _STA_ one of my boys needed to lose some weight and the other did n't . i put this food on the floor for the chubby guy , and the protein-rich , no by-product food up higher where only my skinny boy can jump . the higher food sits going stale . they both really go for this food . and my chubby boy has been losing about an ounce a week .\n",
      "inference: \n",
      "target: my cats love this `` diet '' food better than their regular food _EOS_\n",
      "input: _STA_ my cats have been happily eating felidae platinum for more than two years . i just got a new bag and the shape of the food is different . they tried the new food when i first put it in their bowls and now the bowls sit full and the kitties will not touch the food . i 've noticed similar reviews related to formula changes in the past . unfortunately , i now need to find a new food that my cats will eat .\n",
      "inference: \n",
      "target: my cats are not fans of the new food _EOS_\n",
      "input: _STA_ good flavor ! these came securely packed ... they were fresh and delicious ! i love these twizzlers !\n",
      "inference: zuma\n",
      "target: fresh and greasy ! _EOS_\n",
      "input: _STA_ the strawberry twizzlers are my guilty pleasure - yummy . six pounds will be around for a while with my son and i .\n",
      "inference: runny moca fewer fewer fewer fewer\n",
      "target: strawberry twizzlers - yummy _EOS_\n"
     ]
    }
   ],
   "source": [
    "generations=inference(batch_size, z)\n",
    "sent_str = print_inference(generations)\n",
    "input_str = print_input(x_)\n",
    "for i, j, k in zip(sent_str, input_str, target_print):\n",
    "    print('input: '+j)\n",
    "    print(\"inference: \"+i)\n",
    "    print('target: ' + ' '.join(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "z = Variable(torch.randn([batch_size, latent_size])).cuda()\n",
    "\n",
    "generations=inference(batch_size, z)\n",
    "\n",
    "sent_str = print_inference(generations)\n",
    "\n",
    "for i in sent_str:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 4\n",
    "\n",
    "z1 = torch.randn([latent_size]).numpy()\n",
    "z2 = torch.randn([latent_size]).numpy()\n",
    "\n",
    "interpolation = np.zeros((z1.shape[0], steps + 2))\n",
    "\n",
    "for dim, (s,e) in enumerate(zip(z1,z2)):\n",
    "    interpolation[dim] = np.linspace(s,e,steps+2)\n",
    "    \n",
    "z = Variable(torch.from_numpy(interpolation.T).float()).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = z.size(0)\n",
    "\n",
    "generations=inference(batch_size, z)\n",
    "\n",
    "sent_str = print_inference(generations)\n",
    "\n",
    "for i in sent_str:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(rvae.embedding(x_)[8][:10].cpu().data.numpy())\n",
    "fig.colorbar(cax)\n",
    "plt.savefig('books_read.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
